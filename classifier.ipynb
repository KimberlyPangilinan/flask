{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\My\n",
      "[nltk_data]     Pc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS \n",
    "# import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pymysql\n",
    "import numpy as np\n",
    "import datetime\n",
    "app = Flask(__name__)\n",
    "app.json.sort_keys = False\n",
    "CORS(app) \n",
    "\n",
    "db = pymysql.connect(\n",
    "    host='mysql5049.site4now.net',\n",
    "    user='aa0682_movies',\n",
    "    password='Password1234.',\n",
    "    db='db_aa0682_movies',\n",
    "    connect_timeout=8800,\n",
    "    cursorclass=pymysql.cursors.DictCursor\n",
    ")\n",
    "sql_query= \"\"\"\n",
    "            SELECT \n",
    "                article.article_id, \n",
    "                article.title, \n",
    "                article.author, \n",
    "                article.date, \n",
    "                article.date_added,\n",
    "                article.abstract, \n",
    "                journal.journal, \n",
    "                article.keyword, \n",
    "                files.file_name, \n",
    "                COUNT(CASE WHEN logs.type = 'read' THEN 1 END) AS total_reads,\n",
    "                COUNT(CASE WHEN logs.type = 'download' THEN 1 END) AS total_downloads\n",
    "            FROM \n",
    "                article \n",
    "            LEFT JOIN \n",
    "                journal ON article.journal_id = journal.journal_id \n",
    "            LEFT JOIN \n",
    "                logs ON article.article_id = logs.article_id \n",
    "            LEFT JOIN \n",
    "                files ON article.article_id = files.article_id\n",
    "            GROUP BY\n",
    "                article.article_id \n",
    "     \n",
    "\n",
    "           \"\"\"\n",
    "db.ping(reconnect=True)\n",
    "cursor = db.cursor()\n",
    "cursor.execute(sql_query)\n",
    "data = cursor.fetchall()\n",
    "\n",
    "\n",
    "id = [row['article_id'] for row in data]\n",
    "# overviews_orig = [row['abstract'] for row in data]\n",
    "overviews = [row['abstract'] for row in data]\n",
    "titles = [row['title'] for row in data] \n",
    "# titles_orig = [row['title']  for row in data] \n",
    "# author = [row['author']  for row in data] \n",
    "# keyword = [row['keyword']  for row in data] \n",
    "# date = [row['date']  for row in data] \n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "nltk.download(\"stopwords\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "for n, name in enumerate(overviews):\n",
    "    temp = name.lower().split(\" \")\n",
    "    temp = [''.join([letter for letter in word if letter.isalnum()]) for word in temp]\n",
    "    temp = [word for word in temp if word not in stop_words]\n",
    "    temp = ' '.join(temp)\n",
    "    overviews[n] = temp\n",
    "    \n",
    "for n, title in enumerate(titles):\n",
    "    temp = title.lower().split(\" \")\n",
    "    temp = [''.join([letter for letter in word if letter.isalnum()]) for word in temp]\n",
    "    temp = [word for word in temp if word not in stop_words]\n",
    "    temp = ' '.join(temp)\n",
    "    titles[n] = temp\n",
    "    \n",
    "# Calculate cosine similarity\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "    vectorizer = CountVectorizer().fit(overviews + titles)\n",
    "    # Calculate cosine similarity for overviews\n",
    "    vectorizer_overviews = vectorizer.transform(overviews)\n",
    "    cosine_sim_overviews = cosine_similarity(vectorizer_overviews)\n",
    "\n",
    "    # Calculate cosine similarity for titles\n",
    "    vectorizer_titles =  vectorizer.transform(titles)\n",
    "    cosine_sim_titles = cosine_similarity(vectorizer_titles)\n",
    "    \n",
    "    article_id_to_index = {}  # Create an empty mapping\n",
    "    for index, article_id in enumerate(id):\n",
    "        article_id_to_index[article_id] = index\n",
    "def get_article_recommendations( article_id, overviews_similarity_matrix, titles_similarity_matrix):\n",
    "    combined_similarity = 0.2 * overviews_similarity_matrix + 0.8 * titles_similarity_matrix\n",
    "    \n",
    "    if article_id in article_id_to_index:\n",
    "        index = article_id_to_index[article_id]\n",
    "        similar_articles = combined_similarity[index]\n",
    "        similar_articles = sorted(enumerate(similar_articles), key=lambda x: x[1], reverse=True)\n",
    "        recommended_articles = []\n",
    "        \n",
    "        for i in similar_articles:\n",
    "            if i[1] < 0.25:\n",
    "                break\n",
    "            # recommended_article_title = titles_orig[i[0]]\n",
    "            # article_description = overviews_orig[i[0]]\n",
    "            # recommended_articles.append({'title': recommended_article_title, 'article_id': id[i[0]], 'score': i[1]})\n",
    "            \n",
    "            recommended_article = {key: data[i[0]][key] for key in data[i[0]]}\n",
    "            recommended_article['score'] = i[1]\n",
    "            recommended_articles.append(recommended_article)\n",
    "\n",
    "\n",
    "        return recommended_articles\n",
    "    else:\n",
    "        return [\"Article ID not found in the mapping.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, LearningRateScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "MAXLEN = 250\n",
    "TRUNCATING = 'post'\n",
    "PADDING = 'post'\n",
    "OOV_TOKEN = \"<OOV>\"\n",
    "# MAX_EXAMPLES = 160000\n",
    "TRAINING_SPLIT = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 189 sentences and 189 labels\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bundle the two lists into a single one\n",
    "labels = [ '_'.join(sub['journal'].split(' ')) for n,sub in enumerate(data) ]\n",
    "sentences_and_labels = list(zip(overviews, labels))\n",
    "\n",
    "# # Perform random sampling\n",
    "# random.seed(42)\n",
    "# sentences_and_labels = random.sample(sentences_and_labels, len(overviews))\n",
    "\n",
    "# Unpack back into separate lists\n",
    "sentences, labels = zip(*sentences_and_labels)\n",
    "\n",
    "print(f\"There are {len(sentences)} sentences and {len(labels)} labels\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(sentences, labels, training_split, label_encoder):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and validation sets\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of string): lower-cased sentences without stopwords\n",
    "        labels (list of string): list of labels\n",
    "        training split (float): proportion of the dataset to convert to include in the train set\n",
    "    \n",
    "    Returns:\n",
    "        train_sentences, validation_sentences, train_labels, validation_labels - lists containing the data splits\n",
    "    \"\"\"    \n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Compute the number of sentences that will be used for training (should be an integer)\n",
    "    train_size = int(len(sentences)*training_split)\n",
    "\n",
    "    ## Shuffle Lists\n",
    "    temp = list(zip(sentences,labels))\n",
    "    random.shuffle(temp)\n",
    "    sentences,labels = zip(*temp)\n",
    "\n",
    "    # Convert labels strings to integers\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "    # Split the sentences and labels into train/validation splits\n",
    "    train_sentences = sentences[:train_size]\n",
    "    train_labels = labels[:train_size]\n",
    "\n",
    "    validation_sentences = sentences[train_size:]\n",
    "    validation_labels = labels[train_size:]\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return train_sentences, validation_sentences, train_labels, validation_labels, label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 151 sentences for training.\n",
      "\n",
      "There are 151 labels for training.\n",
      "\n",
      "There are 38 sentences for validation.\n",
      "\n",
      "There are 38 labels for validation.\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "label_encoder = LabelEncoder()\n",
    "train_sentences, val_sentences, train_labels, val_labels, label_encoder = train_val_split(sentences, labels, TRAINING_SPLIT, label_encoder)\n",
    "\n",
    "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
    "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
    "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
    "print(f\"There are {len(val_labels)} labels for validation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_tokenizer(train_sentences, oov_token):\n",
    "    \"\"\"\n",
    "    Instantiates the Tokenizer class on the training sentences\n",
    "    \n",
    "    Args:\n",
    "        train_sentences (list of string): lower-cased sentences without stopwords to be used for training\n",
    "        oov_token (string) - symbol for the out-of-vocabulary token\n",
    "    \n",
    "    Returns:\n",
    "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    # Instantiate the Tokenizer class, passing in the correct value for oov_token\n",
    "    tokenizer = Tokenizer(oov_token=oov_token)\n",
    "    \n",
    "    # Fit the tokenizer to the training sentences\n",
    "    tokenizer.fit_on_texts(train_sentences)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 6283 words\n",
      "\n",
      "<OOV> token included in vocabulary\n",
      "\n",
      "index of word 'gender' should be 304\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "tokenizer = fit_tokenizer(train_sentences, OOV_TOKEN)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "VOCAB_SIZE = len(word_index)\n",
    "\n",
    "print(f\"Vocabulary contains {VOCAB_SIZE} words\\n\")\n",
    "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")\n",
    "print(f\"\\nindex of word 'gender' should be {word_index['gender']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
    "    \"\"\"\n",
    "    Generates an array of token sequences and pads them to the same length\n",
    "    \n",
    "    Args:\n",
    "        sentences (list of string): list of sentences to tokenize and pad\n",
    "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
    "        padding (string): type of padding to use\n",
    "        truncating (string): type of truncating to use\n",
    "        maxlen (int): maximum length of the token sequence\n",
    "    \n",
    "    Returns:\n",
    "        pad_trunc_sequences (array of int): tokenized sentences padded to the same length\n",
    "    \"\"\"        \n",
    "    ### START CODE HERE\n",
    "       \n",
    "    # Convert sentences to sequences\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    \n",
    "    # Pad the sequences using the correct padding, truncating and maxlen\n",
    "    pad_trunc_sequences = pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\n",
    "    \n",
    "    ### END CODE HERE\n",
    "    \n",
    "    return pad_trunc_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded and truncated training sequences have shape: (151, 250)\n",
      "\n",
      "Padded and truncated validation sequences have shape: (38, 250)\n"
     ]
    }
   ],
   "source": [
    "# Test your function\n",
    "train_pad_trunc_seq = seq_pad_and_trunc(train_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
    "val_pad_trunc_seq = seq_pad_and_trunc(val_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
    "\n",
    "print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n",
    "print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = np.array(train_labels)\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to file containing the embeddings\n",
    "GLOVE_FILE = './glove.6B.100d.txt'\n",
    "\n",
    "# Initialize an empty embeddings index dictionary\n",
    "GLOVE_EMBEDDINGS = {}\n",
    "\n",
    "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
    "with open(GLOVE_FILE, encoding=\"utf8\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        GLOVE_EMBEDDINGS[word] = coefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector representation of word gender looks like this:\n",
      "\n",
      "[ 0.7528     0.58694    0.10059    0.45384   -0.59904    0.3756\n",
      " -0.48809    0.3255     0.66021    1.0741    -0.87795   -0.070058\n",
      "  0.38828    0.17249    1.198     -0.14775    0.48218    0.0080089\n",
      " -0.072508  -0.29931    0.050217  -0.7677     0.40769    0.52481\n",
      " -0.33323   -0.70466    0.48616   -1.4025    -0.32132   -0.11926\n",
      " -0.28853    0.29444    0.12296   -0.27784    0.64675   -0.80821\n",
      "  0.24569    0.91006   -0.69788    0.78162   -0.75286   -0.309\n",
      " -0.74415    0.13453   -1.1376    -0.21934   -0.043231   0.90301\n",
      " -1.6366    -0.77089    0.37707   -0.070822  -1.0185     1.0259\n",
      "  0.28272   -1.0935     0.46933   -0.21428    0.63936    0.1496\n",
      " -0.12656   -0.19598   -0.10937   -0.50143    1.2136    -0.046943\n",
      "  0.57951   -0.55169    0.25192    0.62123   -0.12948    0.58623\n",
      " -0.19002    0.056338   0.40258   -0.11338   -0.61292   -0.74152\n",
      " -0.49465   -0.98182   -1.1541    -0.39493    0.82455    0.1865\n",
      " -1.6896    -0.035706  -0.88967   -0.46248    0.42013   -0.28066\n",
      " -0.63143    0.25901   -0.31026   -0.024644   0.69174   -0.03463\n",
      " -0.11744   -0.62681    0.34728    0.41872  ]\n"
     ]
    }
   ],
   "source": [
    "test_word = 'gender'\n",
    "\n",
    "test_vector = GLOVE_EMBEDDINGS[test_word]\n",
    "\n",
    "print(f\"Vector representation of word {test_word} looks like this:\\n\\n{test_vector}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty numpy array with the appropriate size\n",
    "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
    "\n",
    "# Iterate all of the words in the vocabulary and if the vector representation for \n",
    "# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        EMBEDDINGS_MATRIX[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_model\n",
    "def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
    "    \"\"\"\n",
    "    Creates a binary sentiment classifier model\n",
    "    \n",
    "    Args:\n",
    "        vocab_size (int): size of the vocabulary for the Embedding layer input\n",
    "        embedding_dim (int): dimensionality of the Embedding layer output\n",
    "        maxlen (int): length of the input sequences\n",
    "        embeddings_matrix (array): predefined weights of the embeddings\n",
    "    \n",
    "    Returns:\n",
    "        model (tf.keras Model): the sentiment classifier model\n",
    "    \"\"\"\n",
    "    ### START CODE HERE\n",
    "    \n",
    "    model = tf.keras.Sequential([ \n",
    "        # This is how you need to set the Embedding layer when using pre-trained embeddings\n",
    "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False), \n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation=\"relu\", return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, activation=\"relu\")),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(3, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                  optimizer=optimizers.Adam(1e-4),\n",
    "                  metrics=['accuracy']) \n",
    "\n",
    "    ### END CODE HERE\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1,\n",
    "                              patience=3, min_lr=1e-7)\n",
    "es = EarlyStopping(monitor='val_loss', patience=5, mode=\"min\", restore_best_weights=True)\n",
    "def scheduler(epoch, lr):\n",
    "  if epoch < 8:\n",
    "    return lr\n",
    "  else:\n",
    "    return lr * tf.math.exp(-0.1)\n",
    "schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/32\n",
      "5/5 [==============================] - 12s 2s/step - loss: 1.0894 - accuracy: 0.4570 - val_loss: 1.0717 - val_accuracy: 0.7105 - lr: 1.0000e-04\n",
      "Epoch 2/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 1.0509 - accuracy: 0.5629 - val_loss: 1.0423 - val_accuracy: 0.6579 - lr: 1.0000e-04\n",
      "Epoch 3/32\n",
      "5/5 [==============================] - 9s 2s/step - loss: 1.0167 - accuracy: 0.5563 - val_loss: 1.0131 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
      "Epoch 4/32\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.9821 - accuracy: 0.5497 - val_loss: 0.9782 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
      "Epoch 5/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.9417 - accuracy: 0.5563 - val_loss: 0.9383 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
      "Epoch 6/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.9000 - accuracy: 0.5430 - val_loss: 0.8913 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
      "Epoch 7/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.8539 - accuracy: 0.5563 - val_loss: 0.8429 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
      "Epoch 8/32\n",
      "5/5 [==============================] - 9s 2s/step - loss: 0.8270 - accuracy: 0.5563 - val_loss: 0.8121 - val_accuracy: 0.6316 - lr: 1.0000e-04\n",
      "Epoch 9/32\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.7899 - accuracy: 0.5762 - val_loss: 0.7975 - val_accuracy: 0.6316 - lr: 9.0484e-05\n",
      "Epoch 10/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 8.8374 - accuracy: 0.6093 - val_loss: 0.8042 - val_accuracy: 0.6316 - lr: 8.1873e-05\n",
      "Epoch 11/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.7657 - accuracy: 0.6424 - val_loss: 0.8190 - val_accuracy: 0.6316 - lr: 7.4082e-05\n",
      "Epoch 12/32\n",
      "5/5 [==============================] - 7s 2s/step - loss: 0.7832 - accuracy: 0.6358 - val_loss: 0.8264 - val_accuracy: 0.6316 - lr: 6.7032e-06\n",
      "Epoch 13/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.7887 - accuracy: 0.6291 - val_loss: 0.8266 - val_accuracy: 0.6316 - lr: 6.0653e-06\n",
      "Epoch 14/32\n",
      "5/5 [==============================] - 8s 2s/step - loss: 0.7890 - accuracy: 0.6159 - val_loss: 0.8268 - val_accuracy: 0.6316 - lr: 5.4881e-06\n"
     ]
    }
   ],
   "source": [
    "# Create your untrained model\n",
    "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)\n",
    "\n",
    "# Train the model and save the training history\n",
    "history = model.fit(train_pad_trunc_seq, train_labels, epochs=32, validation_data=(val_pad_trunc_seq, val_labels), callbacks=[reduce_lr, es, schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 310ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['The_Gavel'], dtype='<U9')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Input Preprocessing\n",
    "label_encoder.inverse_transform([np.argmax(())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 292ms/step\n",
      "Ground Truth: ['The_Gavel']\n",
      "Classification Truth: ['The_Gavel']\n",
      "Actual Input Sentence\n"
     ]
    }
   ],
   "source": [
    "number = 100\n",
    "test_data = train_pad_trunc_seq[number].reshape(1,250)\n",
    "ground_truth = train_labels[number]\n",
    "classification = model.predict(test_data)\n",
    "\n",
    "print('Ground Truth:', label_encoder.inverse_transform([(ground_truth)]))\n",
    "print('Classification Truth:', label_encoder.inverse_transform([np.argmax((classification))]))\n",
    "\n",
    "print('Actual Input Sentence', )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flask_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
